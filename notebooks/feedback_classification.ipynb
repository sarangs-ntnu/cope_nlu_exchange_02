{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feedback Classification with Traditional ML\n",
        "\n",
        "This notebook trains classical machine-learning models to predict three labels from feedback comments:\n",
        "\n",
        "- **Teacher vs. course**\n",
        "- **Sentiment**\n",
        "- **Aspect** (behaviour, teaching skills, relevancy, etc.)\n",
        "\n",
        "To keep the repository free of binary artifacts, the dataset is versioned as `data/data_feedback.csv`. The first code cell exports it to `data/data_feedback.xlsx` so the rest of the workflow can load the Excel file exactly as originally requested.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "data-prep"
        ]
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import shap\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "data_dir = Path('data')\n",
        "csv_path = data_dir / 'data_feedback.csv'\n",
        "excel_path = data_dir / 'data_feedback.xlsx'\n",
        "\n",
        "# Export CSV to Excel so downstream cells can load the XLSX file without committing a binary to git\n",
        "df_csv = pd.read_csv(csv_path)\n",
        "df_csv.to_excel(excel_path, index=False)\n",
        "print(f\"Excel file created at: {excel_path.resolve()}\")\n",
        "\n",
        "# Load the Excel version requested by the assignment\n",
        "df = pd.read_excel(excel_path)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Label distributions\n",
        "\n",
        "A quick overview of the class balance for each prediction task helps choose evaluation strategies for the small dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "for ax, column in zip(axes, ['teacher/course', 'sentiment', 'aspect']):\n",
        "    sns.countplot(data=df, x=column, ax=ax, palette='crest')\n",
        "    ax.set_title(f\"{column} distribution\")\n",
        "    ax.tick_params(axis='x', rotation=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility: train and evaluate a TF-IDF + Logistic Regression model\n",
        "\n",
        "We reuse the same helper to train separate classifiers for each label. Confusion matrices and misclassifications make errors explicit for later explainability analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train_and_report(df, text_col, target_col, test_size=0.25, random_state=42):\n",
        "    train_df, test_df = train_test_split(df, test_size=test_size, stratify=df[target_col], random_state=random_state)\n",
        "\n",
        "    model = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(ngram_range=(1, 2), min_df=1)),\n",
        "        ('logreg', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
        "    ])\n",
        "\n",
        "    model.fit(train_df[text_col], train_df[target_col])\n",
        "    preds = model.predict(test_df[text_col])\n",
        "\n",
        "    print(f\"=== {target_col} ===\")\n",
        "    print(classification_report(test_df[target_col], preds))\n",
        "\n",
        "    cm = confusion_matrix(test_df[target_col], preds, labels=model.classes_)\n",
        "    fig, ax = plt.subplots(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_, ax=ax)\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('True')\n",
        "    ax.set_title(f\"Confusion Matrix: {target_col}\")\n",
        "    plt.show()\n",
        "\n",
        "    errors = test_df.assign(prediction=preds)[lambda d: d[target_col] != d['prediction']]\n",
        "    return model, errors[['comments', target_col, 'prediction']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train separate models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "teacher_model, teacher_errors = train_and_report(df, 'comments', 'teacher/course')\n",
        "sentiment_model, sentiment_errors = train_and_report(df, 'comments', 'sentiment')\n",
        "aspect_model, aspect_errors = train_and_report(df, 'comments', 'aspect')\n",
        "\n",
        "teacher_errors.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error analysis\n",
        "\n",
        "Inspecting misclassifications highlights where the models struggle. The small dataset makes each error easy to review manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def show_errors(errors, title, n=5):\n",
        "    display(errors.head(n))\n",
        "\n",
        "show_errors(teacher_errors, 'Teacher/Course errors')\n",
        "show_errors(sentiment_errors, 'Sentiment errors')\n",
        "show_errors(aspect_errors, 'Aspect errors')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explainability with SHAP\n\n",
        "We use SHAP values on the teacher/course classifier to understand which n-grams contribute to each prediction. The same approach can be applied to the sentiment or aspect models.\n\n",
        "If SHAP is not installed, run `pip install shap` before executing this section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "explainability"
        ]
      },
      "outputs": [],
      "source": [
        "# Fit a SHAP explainer on the linear model\n",
        "vectorizer = teacher_model.named_steps['tfidf']\n",
        "classifier = teacher_model.named_steps['logreg']\n",
        "\n",
        "# Transform the full corpus for consistent feature mapping\n",
        "X_tfidf = vectorizer.transform(df['comments'])\n",
        "explainer = shap.LinearExplainer(classifier, X_tfidf, feature_perturbation='interventional')\n",
        "shap_values = explainer(X_tfidf)\n",
        "\n",
        "# Visualize feature impact for a few examples\n",
        "shap.summary_plot(shap_values, features=vectorizer.get_feature_names_out(), show=False)\n",
        "plt.title('SHAP summary for teacher/course classifier')\n",
        "plt.show()\n",
        "\n",
        "# Inspect a single comment\n",
        "shap.initjs()\n",
        "shap_text_explainer = shap.Explainer(teacher_model, teacher_model['tfidf'])\n",
        "shap_values_text = shap_text_explainer(df['comments'])\n",
        "shap.plots.text(shap_values_text[0])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}